{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7f4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4118b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './data/train.csv'\n",
    "n_folds = 4\n",
    "min_train_frac = 0.5\n",
    "\n",
    "lag_list      = (1,2,5,10,20,60,120,240,360,480)\n",
    "roll_windows  = (5,20,60,120,240,360)\n",
    "z_windows     = (60,120)          \n",
    "ewma_spans    = (16,32)           \n",
    "rcorr_windows = (20,60,120)\n",
    "rcov_windows  = (20,60,120)\n",
    "rcorr_topk    = 6                 \n",
    "\n",
    "# PCA usage\n",
    "use_pca_y1 = False               \n",
    "use_pca_y2 = False                 \n",
    "pca_components_y2 = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9241832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_top_variance_features(df, base_cols, k=6):\n",
    "    variances = df[base_cols].var().sort_values(ascending=False)\n",
    "    return variances.index[:min(k, len(variances))].tolist()\n",
    "\n",
    "def make_features_full(df,\n",
    "                       lag_list=(1,2,5,10,20,60,120,240,360,480),\n",
    "                       roll_windows=(5,20,60,120,240,360),\n",
    "                       z_windows=(60,120),\n",
    "                       ewma_spans=(16,32),\n",
    "                       rcorr_windows=(20,60,120),\n",
    "                       rcov_windows=(20,60,120),\n",
    "                       rcorr_topk=6):\n",
    "    df = df.copy()\n",
    "    base_cols = [c for c in df.columns if c not in ['time','Y1','Y2']]\n",
    "\n",
    "    # Lags\n",
    "    for lag in lag_list:\n",
    "        for c in base_cols:\n",
    "            df[f\"{c}_lag{lag}\"] = df[c].shift(lag)\n",
    "\n",
    "    # First differences & simple returns (shifted)\n",
    "    for c in base_cols:\n",
    "        df[f\"{c}_diff1\"] = df[c].diff().shift(1)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            df[f\"{c}_ret1\"] = ((df[c] / df[c].shift(1)) - 1.0).shift(1)\n",
    "\n",
    "    # Rolling mean/std on raw and on diffs (shifted)\n",
    "    for w in roll_windows:\n",
    "        for c in base_cols:\n",
    "            s = df[c].shift(1)\n",
    "            df[f\"{c}_rollmean{w}\"] = s.rolling(w).mean()\n",
    "            df[f\"{c}_rollstd{w}\"]  = s.rolling(w).std()\n",
    "            d = df[f\"{c}_diff1\"]\n",
    "            df[f\"{c}_diff1_rollmean{w}\"] = d.rolling(w).mean()\n",
    "            df[f\"{c}_diff1_rollstd{w}\"]  = d.rolling(w).std()\n",
    "\n",
    "    # Rolling z-scores (shifted)\n",
    "    for w in z_windows:\n",
    "        for c in base_cols:\n",
    "            s = df[c].shift(1)\n",
    "            mu = s.rolling(w).mean()\n",
    "            sd = s.rolling(w).std()\n",
    "            df[f\"{c}_z{w}\"] = (s - mu) / sd\n",
    "\n",
    "    # EWMA means (shifted)\n",
    "    for span in ewma_spans:\n",
    "        for c in base_cols:\n",
    "            s = df[c].shift(1)\n",
    "            df[f\"{c}_ewma{span}\"] = s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    # Rolling correlations, covariances, and beta among top-variance features (shifted inputs)\n",
    "    topv = _pick_top_variance_features(df, base_cols, k=rcorr_topk)\n",
    "    for w in rcorr_windows:\n",
    "        for i in range(len(topv)):\n",
    "            for j in range(i+1, len(topv)):\n",
    "                a, b = topv[i], topv[j]\n",
    "                s1 = df[a].shift(1); s2 = df[b].shift(1)\n",
    "                df[f\"rcorr_{a}_{b}_w{w}\"] = s1.rolling(w).corr(s2)\n",
    "\n",
    "    for w in rcov_windows:\n",
    "        for i in range(len(topv)):\n",
    "            for j in range(i+1, len(topv)):\n",
    "                a, b = topv[i], topv[j]\n",
    "                s1 = df[a].shift(1); s2 = df[b].shift(1)\n",
    "                cov = s1.rolling(w).cov(s2)\n",
    "                var_b = s2.rolling(w).var()\n",
    "                beta = cov / var_b\n",
    "                df[f\"rcov_{a}_{b}_w{w}\"]  = cov\n",
    "                df[f\"rbeta_{a}_on_{b}_w{w}\"] = beta\n",
    "\n",
    "    # Drop NaNs created by lags/rolls\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c not in ['Y1','Y2']]\n",
    "    X_all = df[feature_cols]\n",
    "    y1_all = df['Y1']\n",
    "    y2_all = df['Y2']\n",
    "    return X_all, y1_all, y2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21e5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folds_by_time(times, n_folds=4, min_train_frac=0.5):\n",
    "    uniq = np.unique(times)\n",
    "    n = len(uniq)\n",
    "    n_folds = max(2, n_folds)\n",
    "    start_train_idx = int(n * min_train_frac)\n",
    "    remain = n - start_train_idx\n",
    "    seg = max(1, remain // n_folds)\n",
    "    folds = []\n",
    "    for k in range(n_folds):\n",
    "        val_start = start_train_idx + k * seg\n",
    "        val_end   = start_train_idx + (k + 1) * seg if k < n_folds - 1 else n\n",
    "        if val_start >= val_end or val_start >= n:\n",
    "            continue\n",
    "        train_time_max = uniq[val_start - 1]\n",
    "        val_time_start = uniq[val_start]\n",
    "        val_time_end   = uniq[val_end - 1]\n",
    "        train_idx = np.where(times <= train_time_max)[0]\n",
    "        val_mask  = (times >= val_time_start) & (times <= val_time_end)\n",
    "        val_idx   = np.where(val_mask)[0]\n",
    "        if len(train_idx) and len(val_idx):\n",
    "            folds.append((train_idx, val_idx))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a623568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_y1():\n",
    "    models = []\n",
    "    if XGBRegressor is not None:\n",
    "        models.append(('XGBoost', XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=3500,\n",
    "            max_depth=5,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_lambda=3.0,\n",
    "            tree_method='hist',\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )))\n",
    "    if CatBoostRegressor is not None:\n",
    "        models.append(('CatBoost', CatBoostRegressor(\n",
    "            loss_function='RMSE',\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=8.0,\n",
    "            iterations=3000,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08479c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X, y, models, folds, use_pca=False, pca_components=32):\n",
    "    results = {name: [] for name, _ in models}\n",
    "    preds_cache = {name: [] for name, _ in models}  # list of (val_indices, preds) per fold\n",
    "\n",
    "    for fold_id, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        X_tr = X.iloc[tr_idx].copy(); X_va = X.iloc[va_idx].copy()\n",
    "        y_tr = y.iloc[tr_idx].copy(); y_va = y.iloc[va_idx].copy()\n",
    "        feat_cols = [c for c in X_tr.columns if c != 'time']\n",
    "        X_tr_f = X_tr[feat_cols].values; X_va_f = X_va[feat_cols].values\n",
    "\n",
    "        if use_pca:\n",
    "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "            X_tr_s = scaler.fit_transform(X_tr_f)\n",
    "            X_va_s = scaler.transform(X_va_f)\n",
    "            pca = PCA(n_components=min(pca_components, X_tr_s.shape[1]))\n",
    "            X_tr_use = pca.fit_transform(X_tr_s)\n",
    "            X_va_use = pca.transform(X_va_s)\n",
    "        else:\n",
    "            X_tr_use = X_tr_f\n",
    "            X_va_use = X_va_f\n",
    "\n",
    "        for name, model in models:\n",
    "            model.fit(X_tr_use, y_tr)\n",
    "            y_hat = model.predict(X_va_use)\n",
    "            preds_cache[name].append((va_idx, y_hat))\n",
    "            r2 = r2_score(y_va, y_hat)\n",
    "            results[name].append(r2)\n",
    "\n",
    "        print(f\"[fold {fold_id}/{len(folds)}] done\")\n",
    "\n",
    "    return results, preds_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0e25bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (80000, 17)\n",
      "Features shape: (79520, 710)\n",
      "Folds: 4\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "df = pd.read_csv(csv_path).sort_values('time').reset_index(drop=True)\n",
    "print(\"Loaded:\", df.shape)\n",
    "\n",
    "X_all, y1_all, y2_all = make_features_full(\n",
    "    df,\n",
    "    lag_list=lag_list,\n",
    "    roll_windows=roll_windows,\n",
    "    z_windows=z_windows,\n",
    "    ewma_spans=ewma_spans,\n",
    "    rcorr_windows=rcorr_windows,\n",
    "    rcov_windows=rcov_windows,\n",
    "    rcorr_topk=rcorr_topk\n",
    ")\n",
    "print(\"Features shape:\", X_all.shape)\n",
    "\n",
    "times = X_all['time'].values\n",
    "folds = build_folds_by_time(times, n_folds=n_folds, min_train_frac=min_train_frac)\n",
    "print(\"Folds:\", len(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380ec5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1/4] done\n",
      "[fold 2/4] done\n",
      "[fold 3/4] done\n",
      "[fold 4/4] done\n",
      "\n",
      "[Y1] Equal-weight blend (XGB+Cat):\n",
      "Mean R²: 0.7423919326651036\n",
      "Std  R²: 0.03567672216073825\n",
      "Folds : [0.7722805893392132, 0.7797416230064587, 0.6923033147909978, 0.7252422035237454]\n"
     ]
    }
   ],
   "source": [
    "mdl_y1 = models_y1()\n",
    "if len(mdl_y1) == 0:\n",
    "    raise RuntimeError(\"No Y1 models available (need at least XGBoost or CatBoost).\")\n",
    "\n",
    "res_y1, cache_y1 = run_cv(\n",
    "    X_all, y1_all, mdl_y1, folds,\n",
    "    use_pca=use_pca_y1, pca_components=0  # Y1 PCA off\n",
    ")\n",
    "\n",
    "# Build equal-weight blend if both are present; else use the single available model\n",
    "names = [name for name, _ in mdl_y1]\n",
    "if len(names) >= 2:\n",
    "    blend_scores = []\n",
    "    for i, (_, va_idx) in enumerate(folds):\n",
    "        yhats = []\n",
    "        for name in names:\n",
    "            idxs, pred = cache_y1[name][i]\n",
    "            yhats.append(pred)\n",
    "        y_blend = np.mean(np.vstack(yhats), axis=0)\n",
    "        r2 = r2_score(y1_all.iloc[va_idx], y_blend)\n",
    "        blend_scores.append(r2)\n",
    "    print(\"\\n[Y1] Equal-weight blend (XGB+Cat):\")\n",
    "    print(\"Mean R²:\", float(np.mean(blend_scores)))\n",
    "    print(\"Std  R²:\", float(np.std(blend_scores)))\n",
    "    print(\"Folds :\", [float(x) for x in blend_scores])\n",
    "else:\n",
    "    only = names[0]\n",
    "    print(\"\\n[Y1] Single model (no blend available):\", only)\n",
    "    print(\"Mean R²:\", float(np.mean(res_y1[only])))\n",
    "    print(\"Std  R²:\", float(np.std(res_y1[only])))\n",
    "    print(\"Folds :\", [float(x) for x in res_y1[only]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c634a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2_equal_weight_blend(cache, y, folds, names=(\"XGBoost\",\"CatBoost\")):\n",
    "    if not all(n in cache for n in names):\n",
    "        return None\n",
    "    scores = []\n",
    "    for i, (_, va_idx) in enumerate(folds):\n",
    "        yhats = []\n",
    "        for n in names:\n",
    "            idxs, pred = cache[n][i]\n",
    "            yhats.append(pred)\n",
    "        y_blend = np.mean(np.vstack(yhats), axis=0)\n",
    "        scores.append(r2_score(y.iloc[va_idx], y_blend))\n",
    "    return scores\n",
    "\n",
    "def best_convex_weight_1d(cache, y, folds, names=(\"XGBoost\",\"CatBoost\"), grid_step=0.01):\n",
    "    # Build full OOF vectors\n",
    "    X_xgb, X_cat, y_true = [], [], []\n",
    "    for i, (_, va_idx) in enumerate(folds):\n",
    "        _, px = cache[names[0]][i]\n",
    "        _, pc = cache[names[1]][i]\n",
    "        X_xgb.append(px); X_cat.append(pc); y_true.append(y.iloc[va_idx].values)\n",
    "    X_xgb = np.concatenate(X_xgb)\n",
    "    X_cat = np.concatenate(X_cat)\n",
    "    y_true = np.concatenate(y_true)\n",
    "\n",
    "    best_w, best_r2 = 0.5, -1e9\n",
    "    ws = np.arange(0.0, 1.0 + 1e-9, grid_step)\n",
    "    for w in ws:\n",
    "        y_hat = w * X_xgb + (1 - w) * X_cat\n",
    "        r2 = r2_score(y_true, y_hat)\n",
    "        if r2 > best_r2:\n",
    "            best_w, best_r2 = float(w), float(r2)\n",
    "    return best_w, best_r2\n",
    "\n",
    "def apply_convex_weight_per_fold(cache, y, folds, w, names=(\"XGBoost\",\"CatBoost\")):\n",
    "    scores = []\n",
    "    for i, (_, va_idx) in enumerate(folds):\n",
    "        _, px = cache[names[0]][i]\n",
    "        _, pc = cache[names[1]][i]\n",
    "        y_hat = w * px + (1 - w) * pc\n",
    "        scores.append(r2_score(y.iloc[va_idx], y_hat))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce64dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 1/4] done\n",
      "[fold 2/4] done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mdl_y2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Y2 models available (need at least XGBoost or CatBoost).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m res_y2, cache_y2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdl_y2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_pca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pca_y2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_components_y2\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m names \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m mdl_y2]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mset\u001b[39m(names)):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Show base-model CV first\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mrun_cv\u001b[0;34m(X, y, models, folds, use_pca, pca_components)\u001b[0m\n\u001b[1;32m     20\u001b[0m     X_va_use \u001b[38;5;241m=\u001b[39m X_va_f\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_va_use)\n\u001b[1;32m     25\u001b[0m     preds_cache[name]\u001b[38;5;241m.\u001b[39mappend((va_idx, y_hat))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/sklearn.py:1247\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mdl_y2 = models_y2()\n",
    "if len(mdl_y2) == 0:\n",
    "    raise RuntimeError(\"No Y2 models available (need at least XGBoost or CatBoost).\")\n",
    "\n",
    "res_y2, cache_y2 = run_cv(\n",
    "    X_all, y2_all, mdl_y2, folds,\n",
    "    use_pca=use_pca_y2, pca_components=pca_components_y2\n",
    ")\n",
    "\n",
    "names = [name for name, _ in mdl_y2]\n",
    "if set((\"XGBoost\",\"CatBoost\")).issubset(set(names)):\n",
    "    # Show base-model CV first\n",
    "    print(\"\\n[Y2] Base models CV R²:\")\n",
    "    for n in (\"XGBoost\",\"CatBoost\"):\n",
    "        scores = res_y2[n]\n",
    "        print(f\"  {n:8s}: mean={np.mean(scores):.6f}, std={np.std(scores):.6f}, folds={np.round(scores,6)}\")\n",
    "\n",
    "    # Equal-weight blend\n",
    "    eq_scores = y2_equal_weight_blend(cache_y2, y2_all, folds, (\"XGBoost\",\"CatBoost\"))\n",
    "    print(\"\\n[Y2] Equal-weight blend (XGB+Cat):\")\n",
    "    print(\"  mean=\", float(np.mean(eq_scores)), \"std=\", float(np.std(eq_scores)))\n",
    "    print(\"  folds:\", [float(s) for s in eq_scores])\n",
    "\n",
    "    # Convex 1D blend (robust, avoids bad weights)\n",
    "    w, r2_oof = best_convex_weight_1d(cache_y2, y2_all, folds, (\"XGBoost\",\"CatBoost\"), grid_step=0.01)\n",
    "    fold_scores = apply_convex_weight_per_fold(cache_y2, y2_all, folds, w, (\"XGBoost\",\"CatBoost\"))\n",
    "    print(f\"\\n[Y2] Convex blend (XGB weight={w:.2f}, Cat weight={1-w:.2f})\")\n",
    "    print(\"  OOF R² (global):\", r2_oof)\n",
    "    print(\"  CV folds R²     :\", [float(s) for s in fold_scores])\n",
    "    print(\"  mean=\", float(np.mean(fold_scores)), \"std=\", float(np.std(fold_scores)))\n",
    "\n",
    "else:\n",
    "    # Only one model available\n",
    "    only = names[0]\n",
    "    print(\"\\n[Y2] Single model (no blending):\", only)\n",
    "    print(\"  mean=\", float(np.mean(res_y2[only])), \"std=\", float(np.std(res_y2[only])))\n",
    "    print(\"  folds:\", [float(x) for x in res_y2[only]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3957912",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_list      = (1,2,5,10,20,60,120,240)     # no 360/480\n",
    "roll_windows  = (5,20,60,120)                # no 240/360\n",
    "rcorr_windows = (20,60,120)\n",
    "rcov_windows  = (20,60,120)\n",
    "rcorr_topk    = 6\n",
    "use_pca_y2    = False                        # PCA OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87a13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_y2():\n",
    "    models = []\n",
    "    if XGBRegressor is not None:\n",
    "        models.append(('XGBoost', XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=4000,\n",
    "            max_depth=6,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.85,\n",
    "            reg_lambda=4.0,\n",
    "            tree_method='hist',\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )))\n",
    "    if CatBoostRegressor is not None:\n",
    "        models.append(('CatBoost', CatBoostRegressor(\n",
    "            loss_function='RMSE',\n",
    "            learning_rate=0.03,\n",
    "            depth=7,\n",
    "            l2_leaf_reg=10.0,\n",
    "            iterations=3500,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acb21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Y2 — 80/20 Holdout R² ===\n",
      "XGBoost : R² = 0.582033\n",
      "CatBoost: R² = 0.555506\n",
      "Blend50/50: R² = 0.571313\n",
      "Blend w*XGB+(1-w)*Cat (w=1.00): R² = 0.582033\n"
     ]
    }
   ],
   "source": [
    "# === Y2: train models on 80%, evaluate on 20% ===\n",
    "mdl_y2 = models_y2()\n",
    "names  = [name for name, _ in mdl_y2]\n",
    "assert len(mdl_y2) > 0, \"No Y2 models available.\"\n",
    "\n",
    "preds = {}\n",
    "for name, model in mdl_y2:\n",
    "    # remove early stopping kwargs: fit on all train\n",
    "    model.fit(X_tr_use, y_tr)\n",
    "    preds[name] = model.predict(X_te_use)\n",
    "\n",
    "# Report single-model R²\n",
    "print(\"=== Y2 — 80/20 Holdout R² ===\")\n",
    "for name in preds:\n",
    "    r2 = r2_score(y_te, preds[name])\n",
    "    print(f\"{name:8s}: R² = {r2:.6f}\")\n",
    "\n",
    "# Equal-weight blend (if both available)\n",
    "if set((\"XGBoost\",\"CatBoost\")).issubset(set(preds.keys())):\n",
    "    y_eq = 0.5 * preds[\"XGBoost\"] + 0.5 * preds[\"CatBoost\"]\n",
    "    print(f\"{'Blend50/50':8s}: R² = {r2_score(y_te, y_eq):.6f}\")\n",
    "\n",
    "    # Best convex blend weight on test set\n",
    "    ws = np.arange(0.0, 1.0 + 1e-9, 0.01)\n",
    "    best_w, best_r2 = 0.5, -1e9\n",
    "    for w in ws:\n",
    "        y_blend = w * preds[\"XGBoost\"] + (1 - w) * preds[\"CatBoost\"]\n",
    "        r2 = r2_score(y_te, y_blend)\n",
    "        if r2 > best_r2:\n",
    "            best_w, best_r2 = float(w), float(r2)\n",
    "    print(f\"Blend w*XGB+(1-w)*Cat (w={best_w:.2f}): R² = {best_r2:.6f}\")\n",
    "else:\n",
    "    print(\"Blend: only one model available, skipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
